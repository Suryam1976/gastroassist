#!/usr/bin/env python
"""
Evaluation script for GastroAssist summarizer outputs using Groq LLaMA as judge.
This script compares generated summaries to expected/reference summaries and scores
them on multiple dimensions like accuracy, relevance, conciseness, and source usage.

Usage:
    python eval_summarizer.py --test-dir path/to/test/cases --output-file results.json

Requirements:
    - groq
    - pandas
    - tqdm
    - matplotlib
"""

import os
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import time
import matplotlib.pyplot as plt
from typing import Dict, List, Any, Optional, Tuple
import logging
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import Groq for LLaMA evaluation
try:
    from groq import Groq
except ImportError:
    logger.error("Groq package not found. Install with: pip install groq")
    logger.info("Alternatively, modify this script to use a different LLM provider")
    raise

# Load environment variables
load_dotenv()

# Check for GROQ_API_KEY
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    logger.error("GROQ_API_KEY not found in environment variables")
    logger.info("Please set your GROQ_API_KEY to use Groq LLaMA as evaluator")
    raise ValueError("GROQ_API_KEY environment variable is required")

# Initialize Groq client
groq_client = Groq(api_key=GROQ_API_KEY)
DEFAULT_MODEL = "llama3-70b-8192"  # Using LLaMA 3 70B model by default

# Define evaluation criteria
EVAL_CRITERIA = {
    "medical_accuracy": {
        "weight": 0.35,
        "description": "Correctness of medical information and absence of factual errors"
    },
    "relevance": {
        "weight": 0.20,
        "description": "How well the summary addresses the original query"
    },
    "conciseness": {
        "weight": 0.15,
        "description": "Brevity and clarity without unnecessary information"
    },
    "source_usage": {
        "weight": 0.15,
        "description": "Appropriate citation and use of source material"
    },
    "completeness": {
        "weight": 0.15,
        "description": "Coverage of all important aspects from the reference summary"
    }
}

def create_evaluation_prompt(query: str, generated_summary: str, reference_summary: str) -> str:
    """
    Create a prompt for the LLM to evaluate the generated summary against the reference.
    
    Args:
        query: The original gastroenterology query
        generated_summary: The summary produced by GastroAssist
        reference_summary: The reference/expected summary (gold standard)
        
    Returns:
        A structured prompt for the LLM to perform the evaluation
    """
    prompt = """You are an expert gastroenterologist tasked with evaluating AI-generated medical summaries.
You will be provided with:
1. A medical query in the field of gastroenterology
2. A summary generated by an AI system
3. A reference summary (considered the gold standard)

Please evaluate the AI-generated summary on the following criteria:

1. Medical Accuracy (0-10): Correctness of medical information and absence of factual errors
2. Relevance (0-10): How well the summary addresses the original query
3. Conciseness (0-10): Brevity and clarity without unnecessary information
4. Source Usage (0-10): Appropriate citation and use of source material
5. Completeness (0-10): Coverage of all important aspects from the reference summary

For each criterion, provide:
- A score from 0 to 10 (0 = completely inadequate, 10 = perfect)
- A brief explanation of your rating
- For medical accuracy, specifically note any factual errors or omissions

Finally, provide brief qualitative feedback (2-3 sentences) about the overall quality and usefulness of the generated summary for a gastroenterology professional.

Your evaluation should be fair, objective, and focused on clinical utility.

QUERY:
"{query}"

AI-GENERATED SUMMARY:
"{generated_summary}"

REFERENCE SUMMARY:
"{reference_summary}"

FORMAT YOUR RESPONSE EXACTLY AS FOLLOWS:
{{
  "medical_accuracy": {{
    "score": [0-10],
    "explanation": "your explanation here"
  }},
  "relevance": {{
    "score": [0-10],
    "explanation": "your explanation here"
  }},
  "conciseness": {{
    "score": [0-10],
    "explanation": "your explanation here"
  }},
  "source_usage": {{
    "score": [0-10],
    "explanation": "your explanation here"
  }},
  "completeness": {{
    "score": [0-10],
    "explanation": "your explanation here"
  }},
  "overall_feedback": "your 2-3 sentence overall feedback here"
}}

Return ONLY the JSON object and nothing else.
"""
    return prompt.format(
        query=query,
        generated_summary=generated_summary,
        reference_summary=reference_summary
    )

def evaluate_with_llm(query: str, generated_summary: str, reference_summary: str, 
                     model: str = DEFAULT_MODEL, max_retries: int = 3) -> Dict[str, Any]:
    """
    Evaluate the generated summary using Groq's LLaMA model.
    
    Args:
        query: The original gastroenterology query
        generated_summary: The summary produced by GastroAssist
        reference_summary: The reference/expected summary (gold standard)
        model: The Groq model to use for evaluation
        max_retries: Maximum number of retries in case of API errors
        
    Returns:
        Dictionary containing the evaluation results
    """
    prompt = create_evaluation_prompt(query, generated_summary, reference_summary)
    
    for attempt in range(max_retries):
        try:
            response = groq_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert medical evaluator specializing in gastroenterology."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # Lower temperature for more consistent evaluations
                max_tokens=1500
            )
            
            # Extract the content from the response
            eval_text = response.choices[0].message.content.strip()
            
            # Parse the JSON response
            try:
                eval_results = json.loads(eval_text)
                return eval_results
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse LLM response as JSON (attempt {attempt+1}/{max_retries})")
                logger.debug(f"Raw response: {eval_text}")
                if attempt == max_retries - 1:
                    return {
                        "error": "Failed to parse LLM response as JSON",
                        "raw_response": eval_text
                    }
                    
        except Exception as e:
            logger.warning(f"Error calling Groq API (attempt {attempt+1}/{max_retries}): {str(e)}")
            if attempt == max_retries - 1:
                return {
                    "error": f"API error: {str(e)}",
                    "raw_response": None
                }
            time.sleep(2)  # Wait before retrying
    
    return {"error": "Unknown error in evaluation", "raw_response": None}

def calculate_weighted_score(eval_results: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:
    """
    Calculate a weighted score from the evaluation results.
    
    Args:
        eval_results: Evaluation results from the LLM
        
    Returns:
        Tuple containing (weighted_score, individual_scores)
    """
    scores = {}
    
    # Check if there's an error
    if "error" in eval_results:
        return 0.0, {}
    
    # Calculate scores for each criterion
    for criterion, details in EVAL_CRITERIA.items():
        if criterion in eval_results and "score" in eval_results[criterion]:
            raw_score = eval_results[criterion]["score"]
            # Normalize to 0-1 scale
            normalized_score = raw_score / 10.0
            scores[criterion] = normalized_score
    
    # Calculate weighted score
    weighted_score = sum(
        scores.get(criterion, 0) * details["weight"]
        for criterion, details in EVAL_CRITERIA.items()
    )
    
    return weighted_score, scores

def load_test_cases(test_dir: str) -> List[Dict[str, Any]]:
    """
    Load test cases from the specified directory.
    
    Args:
        test_dir: Directory containing test cases
        
    Returns:
        List of test case dictionaries
    """
    test_dir_path = Path(test_dir)
    test_cases = []
    
    if not test_dir_path.exists():
        logger.error(f"Test directory {test_dir} does not exist")
        return []
    
    # Look for JSON files
    json_files = list(test_dir_path.glob("*.json"))
    if json_files:
        for json_file in json_files:
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    test_data = json.load(f)
                    
                if isinstance(test_data, list):
                    test_cases.extend(test_data)
                elif isinstance(test_data, dict):
                    if all(key in test_data for key in ["query", "generated_summary", "reference_summary"]):
                        test_cases.append(test_data)
                    else:
                        # It might be a collection of test cases
                        for key, value in test_data.items():
                            if isinstance(value, dict) and all(k in value for k in ["query", "generated_summary", "reference_summary"]):
                                value["id"] = key
                                test_cases.append(value)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse {json_file} as JSON")
    else:
        # Look for individual files
        query_files = list(test_dir_path.glob("*.query.txt"))
        for query_file in query_files:
            base_name = str(query_file)[:-10]  # Remove ".query.txt"
            gen_file = Path(f"{base_name}.generated.txt")
            ref_file = Path(f"{base_name}.reference.txt")
            
            if gen_file.exists() and ref_file.exists():
                try:
                    with open(query_file, "r", encoding="utf-8") as f:
                        query = f.read().strip()
                    with open(gen_file, "r", encoding="utf-8") as f:
                        generated = f.read().strip()
                    with open(ref_file, "r", encoding="utf-8") as f:
                        reference = f.read().strip()
                    
                    test_cases.append({
                        "id": query_file.stem.replace(".query", ""),
                        "query": query,
                        "generated_summary": generated,
                        "reference_summary": reference
                    })
                except Exception as e:
                    logger.warning(f"Error loading test case from {query_file}: {str(e)}")
    
    return test_cases

def run_evaluations(test_cases: List[Dict[str, Any]], model: str = DEFAULT_MODEL) -> List[Dict[str, Any]]:
    """
    Run evaluations on the test cases.
    
    Args:
        test_cases: List of test cases to evaluate
        model: The LLM model to use for evaluation
        
    Returns:
        List of evaluation results
    """
    results = []
    
    for test_case in tqdm(test_cases, desc="Evaluating summaries"):
        test_id = test_case.get("id", f"test_{len(results)}")
        
        logger.info(f"Evaluating test case: {test_id}")
        eval_result = evaluate_with_llm(
            query=test_case["query"],
            generated_summary=test_case["generated_summary"],
            reference_summary=test_case["reference_summary"],
            model=model
        )
        
        weighted_score, individual_scores = calculate_weighted_score(eval_result)
        
        result = {
            "id": test_id,
            "query": test_case["query"],
            "generated_summary": test_case["generated_summary"],
            "reference_summary": test_case["reference_summary"],
            "evaluation": eval_result,
            "weighted_score": weighted_score,
            "individual_scores": individual_scores,
            "timestamp": datetime.now().isoformat()
        }
        
        results.append(result)
        logger.info(f"Test case {test_id} score: {weighted_score:.4f}")
        
        # Add a short delay to avoid overwhelming the API
        time.sleep(0.5)
    
    return results

def generate_report(results: List[Dict[str, Any]], output_file: str) -> None:
    """
    Generate a report from the evaluation results.
    
    Args:
        results: List of evaluation results
        output_file: Path to save the report
    """
    output_path = Path(output_file)
    output_dir = output_path.parent
    os.makedirs(output_dir, exist_ok=True)
    
    # Save raw JSON results
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2)
    
    # Create pandas DataFrame for analysis
    df = pd.DataFrame([
        {
            "id": r["id"],
            "weighted_score": r["weighted_score"],
            **{
                f"score_{criterion}": r["individual_scores"].get(criterion, np.nan)
                for criterion in EVAL_CRITERIA
            }
        }
        for r in results if "weighted_score" in r and "individual_scores" in r
    ])
    
    # Generate summary statistics
    summary_stats = {
        "total_test_cases": len(results),
        "successful_evaluations": len(df),
        "failed_evaluations": len(results) - len(df),
        "avg_weighted_score": df["weighted_score"].mean() if not df.empty else 0,
        "median_weighted_score": df["weighted_score"].median() if not df.empty else 0,
        "min_weighted_score": df["weighted_score"].min() if not df.empty else 0,
        "max_weighted_score": df["weighted_score"].max() if not df.empty else 0,
        "criteria_averages": {
            criterion: df[f"score_{criterion}"].mean() if not df.empty else 0
            for criterion in EVAL_CRITERIA
        }
    }
    
    # Save summary statistics
    summary_path = output_path.with_suffix(".summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary_stats, f, indent=2)
    
    # Generate CSV report
    csv_path = output_path.with_suffix(".csv")
    if not df.empty:
        df.to_csv(csv_path, index=False)
    
    # Generate visualizations
    if not df.empty and len(df) > 0:
        plot_path = output_path.with_suffix(".png")
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
        
        # Plot 1: Overall score distribution
        df["weighted_score"].hist(bins=10, ax=ax1)
        ax1.set_title("Distribution of Weighted Scores")
        ax1.set_xlabel("Score")
        ax1.set_ylabel("Frequency")
        
        # Plot 2: Average scores by criterion
        criterion_scores = [summary_stats["criteria_averages"][c] for c in EVAL_CRITERIA]
        criterion_names = list(EVAL_CRITERIA.keys())
        ax2.bar(criterion_names, criterion_scores)
        ax2.set_title("Average Scores by Criterion")
        ax2.set_xlabel("Criterion")
        ax2.set_ylabel("Average Score")
        ax2.set_ylim([0, 1.0])
        plt.xticks(rotation=45, ha="right")
        
        plt.tight_layout()
        plt.savefig(plot_path)
        
    logger.info(f"Report saved to {output_path}")
    logger.info(f"Summary saved to {summary_path}")
    if not df.empty:
        logger.info(f"CSV report saved to {csv_path}")
        logger.info(f"Visualization saved to {plot_path}")
    
    # Print summary to console
    print("\n" + "="*50)
    print("EVALUATION SUMMARY")
    print("="*50)
    print(f"Total test cases: {summary_stats['total_test_cases']}")
    print(f"Successful evaluations: {summary_stats['successful_evaluations']}")
    print(f"Failed evaluations: {summary_stats['failed_evaluations']}")
    print(f"Average weighted score: {summary_stats['avg_weighted_score']:.4f}")
    print(f"Median weighted score: {summary_stats['median_weighted_score']:.4f}")
    print(f"Min weighted score: {summary_stats['min_weighted_score']:.4f}")
    print(f"Max weighted score: {summary_stats['max_weighted_score']:.4f}")
    print("\nCriteria Averages:")
    for criterion, score in summary_stats["criteria_averages"].items():
        print(f"  - {criterion}: {score:.4f}")
    print("="*50)

def create_test_case_from_file(query_file: str, generated_file: str, reference_file: str) -> Dict[str, Any]:
    """
    Create a test case from individual files.
    
    Args:
        query_file: Path to file containing the query
        generated_file: Path to file containing the generated summary
        reference_file: Path to file containing the reference summary
        
    Returns:
        Test case dictionary
    """
    try:
        with open(query_file, "r", encoding="utf-8") as f:
            query = f.read().strip()
        with open(generated_file, "r", encoding="utf-8") as f:
            generated = f.read().strip()
        with open(reference_file, "r", encoding="utf-8") as f:
            reference = f.read().strip()
        
        return {
            "id": Path(query_file).stem,
            "query": query,
            "generated_summary": generated,
            "reference_summary": reference
        }
    except Exception as e:
        logger.error(f"Error creating test case: {str(e)}")
        return None

def evaluate_single_summary(query: str, generated_summary: str, reference_summary: str, 
                           model: str = DEFAULT_MODEL) -> Dict[str, Any]:
    """
    Evaluate a single summary.
    
    Args:
        query: The original query
        generated_summary: The generated summary
        reference_summary: The reference summary
        model: The LLM model to use for evaluation
        
    Returns:
        Evaluation results
    """
    eval_result = evaluate_with_llm(query, generated_summary, reference_summary, model)
    weighted_score, individual_scores = calculate_weighted_score(eval_result)
    
    result = {
        "query": query,
        "generated_summary": generated_summary,
        "reference_summary": reference_summary,
        "evaluation": eval_result,
        "weighted_score": weighted_score,
        "individual_scores": individual_scores,
        "timestamp": datetime.now().isoformat()
    }
    
    return result

def create_sample_test_case() -> Dict[str, Any]:
    """
    Create a sample test case for demonstration purposes.
    
    Returns:
        A sample test case
    """
    return {
        "id": "sample_test",
        "query": "What are the current treatment guidelines for H. pylori infection?",
        "generated_summary": "Current guidelines for H. pylori treatment recommend quadruple therapy as first-line treatment in most regions due to increasing clarithromycin resistance. The ACG and AGA recommend bismuth quadruple therapy (PPI, bismuth, tetracycline, and metronidazole) for 14 days. In regions with low clarithromycin resistance, triple therapy with a PPI, clarithromycin, and amoxicillin or metronidazole may still be used. Post-treatment testing to confirm eradication is recommended.",
        "reference_summary": "First-line treatment for H. pylori infection is now bismuth quadruple therapy (PPI, bismuth subcitrate, tetracycline, and metronidazole) for 14 days due to increasing clarithromycin resistance worldwide. In areas with known low clarithromycin resistance (<15%), standard triple therapy with PPI, clarithromycin, and amoxicillin may still be used. Second-line therapies include levofloxacin-based triple therapy or high-dose dual therapy (amoxicillin + PPI). Confirmation of eradication is recommended using urea breath test, stool antigen test, or endoscopic biopsy at least 4 weeks after completion of therapy and at least 2 weeks after PPI discontinuation."
    }

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Evaluate GastroAssist summarizer outputs.")
    parser.add_argument("--test-dir", type=str, help="Directory containing test cases")
    parser.add_argument("--output-file", type=str, default="eval_results.json", help="Output file for evaluation results")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="LLM model to use for evaluation")
    parser.add_argument("--single-test", action="store_true", help="Run a single test case")
    parser.add_argument("--query", type=str, help="Query for single test case")
    parser.add_argument("--generated", type=str, help="Generated summary for single test case")
    parser.add_argument("--reference", type=str, help="Reference summary for single test case")
    parser.add_argument("--create-sample", action="store_true", help="Create a sample test case")
    parser.add_argument("--sample-output", type=str, default="sample_test_case.json", help="Output file for sample test case")
    
    return parser.parse_args()

def main():
    """Main function."""
    args = parse_arguments()
    
    # Create sample test case if requested
    if args.create_sample:
        sample = create_sample_test_case()
        with open(args.sample_output, "w", encoding="utf-8") as f:
            json.dump(sample, f, indent=2)
        logger.info(f"Sample test case saved to {args.sample_output}")
        return
    
    # Run single test case if requested
    if args.single_test:
        if not all([args.query, args.generated, args.reference]):
            logger.error("For single test, you must provide --query, --generated, and --reference")
            return
        
        result = evaluate_single_summary(args.query, args.generated, args.reference, args.model)
        print("\n" + "="*50)
        print("SINGLE TEST EVALUATION")
        print("="*50)
        print(f"Weighted Score: {result['weighted_score']:.4f}")
        print("\nIndividual Scores:")
        for criterion, score in result['individual_scores'].items():
            print(f"  - {criterion}: {score:.4f}")
        print("\nFeedback:")
        if "overall_feedback" in result["evaluation"]:
            print(result["evaluation"]["overall_feedback"])
        print("="*50)
        
        # Save result to file
        with open(args.output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2)
        logger.info(f"Evaluation result saved to {args.output_file}")
        return
    
    # Run full evaluation
    if not args.test_dir:
        logger.error("Please provide --test-dir")
        return
    
    test_cases = load_test_cases(args.test_dir)
    if not test_cases:
        logger.error(f"No test cases found in {args.test_dir}")
        return
    
    logger.info(f"Loaded {len(test_cases)} test cases")
    results = run_evaluations(test_cases, args.model)
    generate_report(results, args.output_file)

if __name__ == "__main__":
    main()
