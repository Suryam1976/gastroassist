# Summarizer Evaluation Guide

This document describes how to evaluate the quality of GastroAssist's medical summaries using the LLM-based evaluation script.

## Overview

The `eval_summarizer.py` script uses Groq LLaMA to evaluate generated summaries against reference summaries, providing scores across multiple dimensions of quality. This allows for quantitative assessment of summary quality and tracking improvements over time.

## Prerequisites

1. **Install required packages**:
   ```bash
   pip install groq pandas tqdm matplotlib
   ```

2. **Set up API key**:
   Add your Groq API key to your `.env` file:
   ```
   GROQ_API_KEY=your_groq_api_key_here
   ```

## Evaluation Criteria

The evaluation script scores summaries on five dimensions:

1. **Medical Accuracy (35% weight)**: Correctness of medical information and absence of factual errors
2. **Relevance (20% weight)**: How well the summary addresses the original query
3. **Conciseness (15% weight)**: Brevity and clarity without unnecessary information
4. **Source Usage (15% weight)**: Appropriate citation and use of source material
5. **Completeness (15% weight)**: Coverage of all important aspects from the reference summary

Each dimension is scored on a scale of 0-10, then normalized to 0-1 and weighted to produce a final score.

## Usage Examples

### Creating a Sample Test Case

To create a sample test case for reference:

```bash
python scripts/eval_summarizer.py --create-sample --sample-output sample_test.json
```

### Evaluating a Single Summary

To evaluate a single summary directly:

```bash
python scripts/eval_summarizer.py --single-test \
  --query "What are the current guidelines for H. pylori treatment?" \
  --generated "Your generated summary" \
  --reference "Your reference summary" \
  --output-file single_eval_result.json
```

### Batch Evaluation

To evaluate multiple test cases at once:

```bash
python scripts/eval_summarizer.py --test-dir manual_testing --output-file eval_results.json
```

The script will search for test cases in the specified directory in the following formats:

1. **JSON files** containing test cases with "query", "generated_summary", and "reference_summary" fields
2. **Text file triplets** with matching names:
   - `example.query.txt`: Contains the query
   - `example.generated.txt`: Contains the generated summary
   - `example.reference.txt`: Contains the reference summary

## Output Files

The script generates multiple output files:

- **JSON results** (`eval_results.json`): Complete evaluation data
- **Summary statistics** (`eval_results.summary.json`): Aggregate statistics
- **CSV report** (`eval_results.csv`): Tabular data for further analysis
- **Visualization** (`eval_results.png`): Charts showing score distribution and averages

## Creating Test Cases

### Manual Test Case Creation

1. Create a JSON file with the following structure:
   ```json
   {
     "id": "test_case_id",
     "query": "The gastroenterology query",
     "generated_summary": "The summary generated by GastroAssist",
     "reference_summary": "The expected/reference summary"
   }
   ```

2. Alternatively, create three separate text files:
   - `test_case_id.query.txt`: Contains the query
   - `test_case_id.generated.txt`: Contains the generated summary
   - `test_case_id.reference.txt`: Contains the reference summary

### Automated Test Case Generation

You can also generate test cases programmatically using the GastroAssist pipeline:

```python
from app.core.query_processor import QueryProcessor
from app.core.reasoning_agent import ReasoningAgent
from app.core.knowledge_router import KnowledgeRouter
import json

# Initialize components
query_processor = QueryProcessor()
reasoning_agent = ReasoningAgent()
knowledge_router = KnowledgeRouter()

# Process a query
query = "What are the latest guidelines for H. pylori treatment?"
processed_query = query_processor.process(query)
information_needs = reasoning_agent.analyze(processed_query)
knowledge_results = knowledge_router.retrieve(information_needs)

# Extract the generated summary
generated_summary = ""
for need_key, need_result in knowledge_results.items():
    if "summarized_response" in need_result and need_result["summarized_response"]:
        if "summary" in need_result["summarized_response"]:
            generated_summary = need_result["summarized_response"]["summary"]
            break

# Create a test case with the generated summary
test_case = {
    "id": "auto_generated_test",
    "query": query,
    "generated_summary": generated_summary,
    "reference_summary": "Your reference summary here"
}

# Save the test case
with open("auto_generated_test.json", "w") as f:
    json.dump(test_case, f, indent=2)
```

## Interpreting Evaluation Results

### Individual Evaluation

Each summary evaluation includes:

- **Scores for each dimension**: Raw (0-10) and normalized (0-1) scores for each criterion
- **Explanations**: Qualitative feedback explaining each score
- **Overall feedback**: General assessment of the summary's quality
- **Weighted score**: Final score combining all dimensions

Example output:

```json
{
  "medical_accuracy": {
    "score": 8,
    "explanation": "The summary correctly identifies bismuth quadruple therapy as the first-line treatment for H. pylori in regions with high clarithromycin resistance. It also correctly mentions the 14-day treatment duration and the need for post-treatment testing. However, it lacks details on second-line therapies and specific eradication testing methods."
  },
  "relevance": {
    "score": 9,
    "explanation": "The summary directly addresses the query about current H. pylori treatment guidelines, focusing specifically on recommended therapies and their rationale."
  },
  "conciseness": {
    "score": 9,
    "explanation": "The text is brief and focused, avoiding unnecessary details while covering the key information in just a few sentences."
  },
  "source_usage": {
    "score": 8,
    "explanation": "The summary appropriately cites authoritative sources (ACG and AGA) for its recommendations, establishing credibility for the information provided."
  },
  "completeness": {
    "score": 7,
    "explanation": "While covering first-line therapies well, the summary omits information about second-line options and specific eradication testing protocols that were included in the reference summary."
  },
  "overall_feedback": "This is a high-quality summary that provides accurate and relevant information about current H. pylori treatment guidelines with appropriate source attribution. The main limitation is the omission of second-line therapy options and specific testing details that would provide a more complete clinical picture."
}
```

### Batch Evaluation Summary

The summary statistics provide an overview of performance across multiple test cases:

- **Average weighted score**: Overall performance
- **Criteria averages**: Performance breakdown by dimension
- **Score distributions**: Visualization of score patterns

Example visualization:

```
================================
EVALUATION SUMMARY
================================
Total test cases: 10
Successful evaluations: 10
Failed evaluations: 0
Average weighted score: 0.8246
Median weighted score: 0.8350
Min weighted score: 0.7580
Max weighted score: 0.8750

Criteria Averages:
  - medical_accuracy: 0.8100
  - relevance: 0.8700
  - conciseness: 0.8800
  - source_usage: 0.7900
  - completeness: 0.7700
================================
```

## Best Practices

1. **Consistent Reference Summaries**: Ensure reference summaries follow a consistent format and quality standard

2. **Representative Test Set**: Create test cases that cover diverse gastroenterology topics and query types

3. **Regular Evaluation**: Run evaluations after significant system changes to track improvements

4. **Score Thresholds**: Define minimum acceptable scores for production readiness (e.g., medical accuracy > 0.85)

5. **Error Analysis**: Review low-scoring summaries to identify patterns and improvement opportunities

## Troubleshooting

### API Key Issues

If you encounter authentication errors:
1. Verify your Groq API key is correctly set in your `.env` file
2. Check that you have access to the LLaMA model specified
3. Ensure your billing information is up to date

### Parsing Errors

If the evaluation script fails to parse LLM responses:
1. Check the raw response in the error output
2. Try reducing batch size to avoid context window limitations
3. Consider using a different LLM model

### High Latency

If evaluations are taking too long:
1. Reduce the number of test cases processed in a single run
2. Consider using a smaller, faster LLM model for initial testing
3. Implement checkpointing to save progress during large batch runs

## Integration with CI/CD

You can integrate the evaluation script into your CI/CD pipeline:

```yaml
# Example GitHub Actions workflow step
- name: Evaluate Summarizer
  run: |
    python -m pip install -r requirements.txt
    python scripts/eval_summarizer.py --test-dir ./test_cases --output-file ./results/eval_results.json
    python scripts/check_eval_threshold.py --min-score 0.80 --results ./results/eval_results.json
```

## Extending the Evaluation

The evaluation framework can be extended in several ways:

1. **Additional Criteria**: Add new evaluation dimensions (e.g., medical terminology usage)

2. **Different LLM Providers**: Replace Groq with other LLM providers

3. **Human Validation**: Add functionality to compare LLM evaluations with human expert evaluations

4. **Multi-Model Consensus**: Use multiple LLMs and average their evaluations for more robust results
